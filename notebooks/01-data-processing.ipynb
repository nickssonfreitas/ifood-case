{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6381b590-08c8-405d-9fd0-a314c486de49",
   "metadata": {},
   "source": [
    "# üìä Notebook 1 ‚Äî Processamento e Prepara√ß√£o de Dados\n",
    "\n",
    "Este notebook tem como objetivo realizar a **explora√ß√£o, limpeza e prepara√ß√£o dos dados brutos** do case t√©cnico proposto, utilizando **PySpark** como engine principal de processamento distribu√≠do.\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Objetivo\n",
    "\n",
    "- Carregar os dados brutos fornecidos (`customers.json.gz`, `offers.json.gz`, `transactions.json.gz`)\n",
    "- Realizar uma an√°lise explorat√≥ria inicial (**EDA**) para entender a estrutura, qualidade e distribui√ß√£o dos dados\n",
    "- Tratar dados faltantes, tipos e formatos\n",
    "- Preparar um conjunto de dados unificado e otimizado para an√°lise e modelagem futura\n",
    "- Salvar os dados tratados em formato **Parquet**, que √© mais eficiente e leve para uso com Spark\n",
    "\n",
    "---\n",
    "\n",
    "## üîÅ Etapas executadas neste notebook\n",
    "\n",
    "1. Importa√ß√£o de bibliotecas e configura√ß√£o do ambiente PySpark\n",
    "2. Leitura dos arquivos `.json.gz` diretamente com Spark\n",
    "3. Explora√ß√£o e valida√ß√£o de schemas e estat√≠sticas dos dados\n",
    "4. Tratamento de dados ausentes, inconsistentes ou inv√°lidos\n",
    "5. Convers√£o para formatos otimizados (`.parquet`)\n",
    "6. Exporta√ß√£o dos dados tratados para `data/processed/`\n",
    "\n",
    "---\n",
    "\n",
    "## üóÇÔ∏è Estrutura esperada dos dados\n",
    "\n",
    "- `data/raw/` ‚Üí Arquivos `.json.gz` (compactados)\n",
    "- `data/processed/` ‚Üí Arquivos `.parquet` tratados e otimizados\n",
    "\n",
    "---\n",
    "\n",
    "## ‚öôÔ∏è Tecnologias utilizadas\n",
    "\n",
    "- Python 3.11\n",
    "- PySpark\n",
    "- JupyterLab\n",
    "- Pandas (suporte auxiliar para an√°lise explorat√≥ria)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4982ded1-364a-414f-98cb-808087dbe2b5",
   "metadata": {},
   "source": [
    "## 1. üì¶ Importa√ß√£o de bibliotecas e configura√ß√£o do PySpark\n",
    "\n",
    "Nesta etapa, vamos:\n",
    "\n",
    "- Importar as bibliotecas necess√°rias para manipula√ß√£o e an√°lise dos dados\n",
    "- Inicializar a sess√£o do PySpark (`SparkSession`), que ser√° usada para leitura, transforma√ß√£o e grava√ß√£o dos dados\n",
    "- Configurar par√¢metros b√°sicos de execu√ß√£o, como nome da aplica√ß√£o e quantidade de mem√≥ria (caso necess√°rio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19987cee-1254-42c6-a6c9-0e311043f36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Obt√©m o caminho absoluto do diret√≥rio 'src'\n",
    "src_path = os.path.abspath(\"../\")\n",
    "# Adiciona 'src' ao sys.path\n",
    "if src_path not in sys.path:\n",
    "    sys.path.append(src_path)\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when, isnan, count\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f5d922-f3f8-434e-b504-14a75864093c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0dd7c6-5ba0-4ee5-b415-c920b9bd24fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializa√ß√£o do SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"iFood - Data Processing\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "#Testa se Spark est√° funcionando\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "print(\"‚úÖ SparkSession iniciada com sucesso!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c030c96-251b-4702-bafa-04a302652a0d",
   "metadata": {},
   "source": [
    "## 2.Leitura dos dados brutos (.json.gz)\n",
    "\n",
    "Nesta etapa, vamos:\n",
    "\n",
    "- Ler os tr√™s conjuntos de dados fornecidos:\n",
    "  - `customers.json.gz`\n",
    "  - `offers.json.gz`\n",
    "  - `transactions.json.gz`\n",
    "- Utilizar o PySpark para carregar os arquivos diretamente do formato `.gz`, que √© suportado nativamente\n",
    "- Exibir uma pr√©via de cada dataset e verificar seu schema para garantir que os dados foram carregados corretamente\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c06caeb9-21a5-472d-8b8f-f28016000388",
   "metadata": {},
   "outputs": [],
   "source": [
    "# L√™ os arquivos JSON compactados com PySpark\n",
    "customers_df = spark.read.json(\"/app/data/raw/profile.json.gz\")\n",
    "offers_df = spark.read.json(\"/app/data/raw/offers.json.gz\")\n",
    "transactions_df = spark.read.json(\"/app/data/raw/transactions.json.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24781c02-bef3-4f90-a225-5b6ab59c4e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostra uma pr√©via dos dados\n",
    "print(\"üè∑Ô∏è Offers:\")\n",
    "offers_df.show(5, truncate=False)\n",
    "\n",
    "print(\"üë§ Customers:\")\n",
    "customers_df.show(5, truncate=False)\n",
    "\n",
    "print(\"üí≥ Transactions:\")\n",
    "transactions_df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b547293-ddef-4d7e-b11d-c2b63a6ea294",
   "metadata": {},
   "source": [
    "## 3. üîç Explora√ß√£o e Valida√ß√£o dos Dados (EDA)\n",
    "\n",
    "Nesta etapa, vamos:\n",
    "\n",
    "- Explorar os schemas das tabelas para verificar tipos de dados\n",
    "- Observar estat√≠sticas descritivas b√°sicas\n",
    "- Contar valores nulos e valores √∫nicos\n",
    "- Identificar poss√≠veis problemas de qualidade (ex: campos vazios, inconsistentes)\n",
    "- Verificar distribui√ß√µes de colunas importantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0506918f-bfe2-4a8d-adaa-65ec8f968867",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.eda.data_diagnostics import isna_sum, value_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0436d17d-123e-4e4d-b5ac-1290cb217ecf",
   "metadata": {},
   "source": [
    "### 3.1 Check NaN Values ``customers``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea2e706-b817-43d6-b3fb-4adc076004c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "isna_sum(customers_df, \"customers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c463cdff-9a75-445d-a8f8-a4d441c0a6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "value_counts(customers_df, \"gender\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f2eb4b-b7ce-4bf4-a892-eebf024a8429",
   "metadata": {},
   "outputs": [],
   "source": [
    "value_counts(customers_df, \"credit_card_limit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2e869b-037d-42d7-8cac-e8dcf4a4969c",
   "metadata": {},
   "source": [
    "### 3.2 Check NaN Values ``offers``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e17515c-5bcb-4a47-a058-1412bd0625c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#isna_sum(offers_df, \"offers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2a3ff9-3a19-494c-8272-a292e18883e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check_schema_and_nulls(transactions_df, \"transactions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9c6ace-0b47-406f-9b83-7b0ebf97b8b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2107d1d8-136e-450c-9d5b-f0abce80c2d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
