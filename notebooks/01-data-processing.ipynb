{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6381b590-08c8-405d-9fd0-a314c486de49",
   "metadata": {},
   "source": [
    "# 📊 Notebook 1 — Processamento e Preparação de Dados\n",
    "\n",
    "Este notebook tem como objetivo realizar a **exploração, limpeza e preparação dos dados brutos** do case técnico proposto, utilizando **PySpark** como engine principal de processamento distribuído.\n",
    "\n",
    "---\n",
    "\n",
    "## 🎯 Objetivo\n",
    "\n",
    "- Carregar os dados brutos fornecidos (`customers.json.gz`, `offers.json.gz`, `transactions.json.gz`)\n",
    "- Realizar uma análise exploratória inicial (**EDA**) para entender a estrutura, qualidade e distribuição dos dados\n",
    "- Tratar dados faltantes, tipos e formatos\n",
    "- Preparar um conjunto de dados unificado e otimizado para análise e modelagem futura\n",
    "- Salvar os dados tratados em formato **Parquet**, que é mais eficiente e leve para uso com Spark\n",
    "\n",
    "---\n",
    "\n",
    "## 🔁 Etapas executadas neste notebook\n",
    "\n",
    "1. Importação de bibliotecas e configuração do ambiente PySpark\n",
    "2. Leitura dos arquivos `.json.gz` diretamente com Spark\n",
    "3. Exploração e validação de schemas e estatísticas dos dados\n",
    "4. Tratamento de dados ausentes, inconsistentes ou inválidos\n",
    "5. Conversão para formatos otimizados (`.parquet`)\n",
    "6. Exportação dos dados tratados para `data/processed/`\n",
    "\n",
    "---\n",
    "\n",
    "## 🗂️ Estrutura esperada dos dados\n",
    "\n",
    "- `data/raw/` → Arquivos `.json.gz` (compactados)\n",
    "- `data/processed/` → Arquivos `.parquet` tratados e otimizados\n",
    "\n",
    "---\n",
    "\n",
    "## ⚙️ Tecnologias utilizadas\n",
    "\n",
    "- Python 3.11\n",
    "- PySpark\n",
    "- JupyterLab\n",
    "- Pandas (suporte auxiliar para análise exploratória)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4982ded1-364a-414f-98cb-808087dbe2b5",
   "metadata": {},
   "source": [
    "## 1. 📦 Importação de bibliotecas e configuração do PySpark\n",
    "\n",
    "Nesta etapa, vamos:\n",
    "\n",
    "- Importar as bibliotecas necessárias para manipulação e análise dos dados\n",
    "- Inicializar a sessão do PySpark (`SparkSession`), que será usada para leitura, transformação e gravação dos dados\n",
    "- Configurar parâmetros básicos de execução, como nome da aplicação e quantidade de memória (caso necessário)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19987cee-1254-42c6-a6c9-0e311043f36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Obtém o caminho absoluto do diretório 'src'\n",
    "src_path = os.path.abspath(\"../\")\n",
    "# Adiciona 'src' ao sys.path\n",
    "if src_path not in sys.path:\n",
    "    sys.path.append(src_path)\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when, isnan, count\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89f5d922-f3f8-434e-b504-14a75864093c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/usr/local/lib/python311.zip',\n",
       " '/usr/local/lib/python3.11',\n",
       " '/usr/local/lib/python3.11/lib-dynload',\n",
       " '',\n",
       " '/usr/local/lib/python3.11/site-packages',\n",
       " '/app']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa0dd7c6-5ba0-4ee5-b415-c920b9bd24fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/03/25 23:09:36 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ SparkSession iniciada com sucesso!\n"
     ]
    }
   ],
   "source": [
    "# Inicialização do SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"iFood - Data Processing\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "#Testa se Spark está funcionando\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "print(\"✅ SparkSession iniciada com sucesso!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c030c96-251b-4702-bafa-04a302652a0d",
   "metadata": {},
   "source": [
    "## 2.Leitura dos dados brutos (.json.gz)\n",
    "\n",
    "Nesta etapa, vamos:\n",
    "\n",
    "- Ler os três conjuntos de dados fornecidos:\n",
    "  - `customers.json.gz`\n",
    "  - `offers.json.gz`\n",
    "  - `transactions.json.gz`\n",
    "- Utilizar o PySpark para carregar os arquivos diretamente do formato `.gz`, que é suportado nativamente\n",
    "- Exibir uma prévia de cada dataset e verificar seu schema para garantir que os dados foram carregados corretamente\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c06caeb9-21a5-472d-8b8f-f28016000388",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Lê os arquivos JSON compactados com PySpark\n",
    "customers_df = spark.read.json(\"/app/data/raw/profile.json.gz\")\n",
    "offers_df = spark.read.json(\"/app/data/raw/offers.json.gz\")\n",
    "transactions_df = spark.read.json(\"/app/data/raw/transactions.json.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24781c02-bef3-4f90-a225-5b6ab59c4e22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏷️ Offers:\n",
      "+----------------------------+--------------+--------+--------------------------------+---------+-------------+\n",
      "|channels                    |discount_value|duration|id                              |min_value|offer_type   |\n",
      "+----------------------------+--------------+--------+--------------------------------+---------+-------------+\n",
      "|[email, mobile, social]     |10            |7.0     |ae264e3637204a6fb9bb56bc8210ddfd|10       |bogo         |\n",
      "|[web, email, mobile, social]|10            |5.0     |4d5c57ea9a6940dd891ad53e9dbe8da0|10       |bogo         |\n",
      "|[web, email, mobile]        |0             |4.0     |3f207df678b143eea3cee63160fa8bed|0        |informational|\n",
      "|[web, email, mobile]        |5             |7.0     |9b98b8c7a33c4b65b9aebfe6a799e6d9|5        |bogo         |\n",
      "|[web, email]                |5             |10.0    |0b1e1539f2cc45b7b9fa7c272da2e1d7|20       |discount     |\n",
      "+----------------------------+--------------+--------+--------------------------------+---------+-------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "👤 Customers:\n",
      "+---+-----------------+------+--------------------------------+-------------+\n",
      "|age|credit_card_limit|gender|id                              |registered_on|\n",
      "+---+-----------------+------+--------------------------------+-------------+\n",
      "|118|NULL             |NULL  |68be06ca386d4c31939f3a4f0e3dd783|20170212     |\n",
      "|55 |112000.0         |F     |0610b486422d4921ae7d2bf64640c50b|20170715     |\n",
      "|118|NULL             |NULL  |38fe809add3b4fcf9315a9694bb96ff5|20180712     |\n",
      "|75 |100000.0         |F     |78afa995795e4d85b5d9ceeca43f5fef|20170509     |\n",
      "|118|NULL             |NULL  |a03223e636434f42ac4c3df47e8bac43|20170804     |\n",
      "+---+-----------------+------+--------------------------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "💳 Transactions:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5:>                                                          (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------+--------------+---------------------+----------------------------------------------------+\n",
      "|account_id                      |event         |time_since_test_start|value                                               |\n",
      "+--------------------------------+--------------+---------------------+----------------------------------------------------+\n",
      "|78afa995795e4d85b5d9ceeca43f5fef|offer received|0.0                  |{NULL, 9b98b8c7a33c4b65b9aebfe6a799e6d9, NULL, NULL}|\n",
      "|a03223e636434f42ac4c3df47e8bac43|offer received|0.0                  |{NULL, 0b1e1539f2cc45b7b9fa7c272da2e1d7, NULL, NULL}|\n",
      "|e2127556f4f64592b11af22de27a7932|offer received|0.0                  |{NULL, 2906b810c7d4411798c6938adc9daaa5, NULL, NULL}|\n",
      "|8ec6ce2a7e7949b1bf142def7d0e0586|offer received|0.0                  |{NULL, fafdcd668e3743c1bb461111dcafc2a4, NULL, NULL}|\n",
      "|68617ca6246f4fbc85e91a2a49552598|offer received|0.0                  |{NULL, 4d5c57ea9a6940dd891ad53e9dbe8da0, NULL, NULL}|\n",
      "+--------------------------------+--------------+---------------------+----------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Mostra uma prévia dos dados\n",
    "print(\"🏷️ Offers:\")\n",
    "offers_df.show(5, truncate=False)\n",
    "\n",
    "print(\"👤 Customers:\")\n",
    "customers_df.show(5, truncate=False)\n",
    "\n",
    "print(\"💳 Transactions:\")\n",
    "transactions_df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b547293-ddef-4d7e-b11d-c2b63a6ea294",
   "metadata": {},
   "source": [
    "## 3. 🔍 Exploração e Validação dos Dados (EDA)\n",
    "\n",
    "Nesta etapa, vamos:\n",
    "\n",
    "- Explorar os schemas das tabelas para verificar tipos de dados\n",
    "- Observar estatísticas descritivas básicas\n",
    "- Contar valores nulos e valores únicos\n",
    "- Identificar possíveis problemas de qualidade (ex: campos vazios, inconsistentes)\n",
    "- Verificar distribuições de colunas importantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0506918f-bfe2-4a8d-adaa-65ec8f968867",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.eda.data_diagnostics import isna_sum, value_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0436d17d-123e-4e4d-b5ac-1290cb217ecf",
   "metadata": {},
   "source": [
    "### 3.1 Check and Fix NaN Values ``customers``\n",
    "- age: **0 nulos (0.00%)**\n",
    "- credit_card_limit: **2175 nulos (12.79%)**\n",
    "- gender: **2175 nulos (12.79%)**\n",
    "- id: **0 nulos (0.00%)**\n",
    "- registered_on: **0 nulos (0.00%)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fea2e706-b817-43d6-b3fb-4adc076004c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📘 Schema de customers:\n",
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- credit_card_limit: double (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- registered_on: string (nullable = true)\n",
      "\n",
      "\n",
      "🔢 Total de linhas: 17000\n",
      "\n",
      "📊 Nulos por coluna (valores e %):\n",
      "– age: 0 nulos (0.00%)\n",
      "– credit_card_limit: 2175 nulos (12.79%)\n",
      "– gender: 2175 nulos (12.79%)\n",
      "– id: 0 nulos (0.00%)\n",
      "– registered_on: 0 nulos (0.00%)\n",
      "\n",
      "🔎 Amostra de customers:\n",
      "+---+-----------------+------+--------------------------------+-------------+\n",
      "|age|credit_card_limit|gender|id                              |registered_on|\n",
      "+---+-----------------+------+--------------------------------+-------------+\n",
      "|118|NULL             |NULL  |68be06ca386d4c31939f3a4f0e3dd783|20170212     |\n",
      "|55 |112000.0         |F     |0610b486422d4921ae7d2bf64640c50b|20170715     |\n",
      "|118|NULL             |NULL  |38fe809add3b4fcf9315a9694bb96ff5|20180712     |\n",
      "|75 |100000.0         |F     |78afa995795e4d85b5d9ceeca43f5fef|20170509     |\n",
      "|118|NULL             |NULL  |a03223e636434f42ac4c3df47e8bac43|20170804     |\n",
      "+---+-----------------+------+--------------------------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "isna_sum(customers_df, \"customers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c50bec7-b1b1-43fb-8561-45d313fcb09b",
   "metadata": {},
   "source": [
    "- Limpeza na Tabela customers\n",
    "1. Coluna gender: Colocar o gender \"O\" para \"NULL\"\n",
    "2. Coluna credit_card_limit: Usar a mediana para substituir os dados nulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c463cdff-9a75-445d-a8f8-a4d441c0a6c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Distribuição da coluna: gender (total: 17000 registros)\n",
      "+------+-----+------------------+\n",
      "|gender|count|percent           |\n",
      "+------+-----+------------------+\n",
      "|M     |8484 |49.90588235294118 |\n",
      "|F     |6129 |36.05294117647059 |\n",
      "|NULL  |2175 |12.794117647058822|\n",
      "|O     |212  |1.2470588235294118|\n",
      "+------+-----+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "value_counts(customers_df, \"gender\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d9f2eb4b-b7ce-4bf4-a892-eebf024a8429",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Distribuição da coluna: credit_card_limit (total: 17000 registros)\n",
      "+-----------------+-----+------------------+\n",
      "|credit_card_limit|count|percent           |\n",
      "+-----------------+-----+------------------+\n",
      "|NULL             |2175 |12.794117647058822|\n",
      "|73000.0          |314  |1.8470588235294116|\n",
      "|72000.0          |297  |1.7470588235294116|\n",
      "|71000.0          |294  |1.7294117647058824|\n",
      "|57000.0          |288  |1.6941176470588233|\n",
      "|74000.0          |282  |1.6588235294117646|\n",
      "|53000.0          |282  |1.6588235294117646|\n",
      "|52000.0          |281  |1.6529411764705884|\n",
      "|56000.0          |281  |1.6529411764705884|\n",
      "|54000.0          |272  |1.6               |\n",
      "|70000.0          |270  |1.588235294117647 |\n",
      "|51000.0          |268  |1.576470588235294 |\n",
      "|61000.0          |258  |1.5176470588235296|\n",
      "|64000.0          |258  |1.5176470588235296|\n",
      "|55000.0          |254  |1.4941176470588236|\n",
      "|50000.0          |253  |1.4882352941176469|\n",
      "|60000.0          |251  |1.4764705882352942|\n",
      "|75000.0          |243  |1.4294117647058824|\n",
      "|59000.0          |243  |1.4294117647058824|\n",
      "|67000.0          |242  |1.423529411764706 |\n",
      "+-----------------+-----+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/25 23:16:42 WARN JavaUtils: Attempt to delete using native Unix OS command failed for path = /tmp/blockmgr-de9a4af0-70c8-4952-8666-986a8408eeb0. Falling back to Java IO way\n",
      "java.io.IOException: Failed to delete: /tmp/blockmgr-de9a4af0-70c8-4952-8666-986a8408eeb0\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingUnixNative(JavaUtils.java:174)\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:109)\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:90)\n",
      "\tat org.apache.spark.util.SparkFileUtils.deleteRecursively(SparkFileUtils.scala:121)\n",
      "\tat org.apache.spark.util.SparkFileUtils.deleteRecursively$(SparkFileUtils.scala:120)\n",
      "\tat org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1126)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.$anonfun$doStop$1(DiskBlockManager.scala:368)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.$anonfun$doStop$1$adapted(DiskBlockManager.scala:364)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.doStop(DiskBlockManager.scala:364)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.stop(DiskBlockManager.scala:359)\n",
      "\tat org.apache.spark.storage.BlockManager.stop(BlockManager.scala:2122)\n",
      "\tat org.apache.spark.SparkEnv.stop(SparkEnv.scala:95)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$stop$25(SparkContext.scala:2305)\n",
      "\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1375)\n",
      "\tat org.apache.spark.SparkContext.stop(SparkContext.scala:2305)\n",
      "\tat org.apache.spark.SparkContext.stop(SparkContext.scala:2211)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$new$34(SparkContext.scala:681)\n",
      "\tat org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:188)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:188)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n"
     ]
    }
   ],
   "source": [
    "value_counts(customers_df, \"credit_card_limit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2e869b-037d-42d7-8cac-e8dcf4a4969c",
   "metadata": {},
   "source": [
    "### 3.2 Check NaN Values ``offers``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e17515c-5bcb-4a47-a058-1412bd0625c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#isna_sum(offers_df, \"offers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2a3ff9-3a19-494c-8272-a292e18883e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check_schema_and_nulls(transactions_df, \"transactions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9c6ace-0b47-406f-9b83-7b0ebf97b8b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2107d1d8-136e-450c-9d5b-f0abce80c2d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
