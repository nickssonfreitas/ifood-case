from pyspark.sql import DataFrame
from pyspark.sql.functions import col, when, mean, year, to_date

from pyspark.sql import DataFrame
from pyspark.sql.functions import col, to_date, year, when


def feature_engineering_customers_data(df: DataFrame) -> DataFrame:
    """
    Realiza engenharia de atributos no DataFrame de clientes.

    Cria:
    - 'birth_year': ano de nascimento com base em 'registered_on' e 'age'
    - 'age_group': categorizaÃ§Ã£o geracional com faixa etÃ¡ria estimada
    - 'credit_limit_bucket': discretizaÃ§Ã£o do limite do cartÃ£o de crÃ©dito

    Faixas do limite de crÃ©dito:
    -----------------------------
    - Low (â‰¤ 10k)
    - Medium (10kâ€“30k)
    - High (30kâ€“60k)
    - Very High (> 60k)

    ParÃ¢metros:
    -----------
    df : DataFrame
        DataFrame do PySpark com colunas:
        - age (int)
        - registered_on (yyyyMMdd)
        - credit_card_limit (float)

    Retorno:
    --------
    DataFrame com as colunas 'birth_year', 'age_group' e 'credit_limit_bucket'
    """

    # CÃ¡lculo do ano de nascimento
    df = df.withColumn(
        "birth_year",
        year(to_date(col("registered_on").cast("string"), "yyyyMMdd")) - col("age")
    )

    # CategorizaÃ§Ã£o de faixa etÃ¡ria
    df = df.withColumn(
        "age_group",
        when(col("birth_year") <= 1964, "Boomers (60+)")
        .when((col("birth_year") >= 1965) & (col("birth_year") <= 1979), "Gen X (45â€“59)")
        .when((col("birth_year") >= 1980) & (col("birth_year") <= 1994), "Millennials (30â€“44)")
        .when((col("birth_year") >= 1995) & (col("birth_year") <= 2009), "Gen Z (15â€“29)")
        .when(col("birth_year") >= 2010, "Gen Alpha (<15)")
        .otherwise("Unknown")
    )

    # Faixa de limite do cartÃ£o de crÃ©dito
    df = df.withColumn(
        "credit_limit_bucket",
        when(col("credit_card_limit") <= 10000, "Low (â‰¤ 10k)")
        .when((col("credit_card_limit") > 10000) & (col("credit_card_limit") <= 30000), "Medium (10kâ€“30k)")
        .when((col("credit_card_limit") > 30000) & (col("credit_card_limit") <= 60000), "High (30kâ€“60k)")
        .when(col("credit_card_limit") > 60000, "Very High (> 60k)")
        .otherwise("Unknown")
    )

    return df


def clean_customers_data(df: DataFrame) -> DataFrame:
    """
    Cleans the 'gender' and 'credit_card_limit' columns in the customers dataset.

    Cleaning rules:
    - 'gender':
        - Keep 'M' and 'F'
        - Replace 'O' and NULL with 'unknown'
    - 'credit_card_limit':
        - Replace NULLs with the median value of the column

    Parameters:
        df (DataFrame): Input Spark DataFrame with raw customer data

    Returns:
        DataFrame: Cleaned DataFrame
    """

    print("ğŸ” Cleaning 'gender' column...")
    df = df.withColumn(
        "gender",
        when(col("gender").isin("M", "F"), col("gender")).otherwise("unknown")
    )
    print("âœ… 'gender' cleaned: values normalized (M, F, unknown)")

    print("\nğŸ“ˆ Calculating statistics for 'credit_card_limit'...")

    # Mean (optional debug/info)
    mean_value = df.select(mean("credit_card_limit")).first()[0]
    print(f"ğŸ“Š Mean credit limit: {mean_value:.2f}")

    # Median via approxQuantile
    median_value = df.approxQuantile("credit_card_limit", [0.5], 0.01)[0]
    print(f"ğŸ“ Median credit limit: {median_value:.2f}")

    # Fill NULLs with median
    df = df.fillna({"credit_card_limit": median_value})
    print("âœ… 'credit_card_limit' nulls filled with median.")

    return df
