from pyspark.sql.functions import col, when, mean
from pyspark.sql import DataFrame
from pyspark.sql.functions import col, count, isnan, when, lit


def isna_sum(df: DataFrame, name: str) -> None:
    """
    Exibe o schema, total de linhas e contagem de nulos + percentual por coluna.

    ParÃ¢metros:
        df (DataFrame): DataFrame do Spark
        name (str): Nome para exibiÃ§Ã£o
    """
    total_rows = df.count()

    print(f"\nðŸ“˜ Schema de {name}:")
    df.printSchema()

    print(f"\nðŸ”¢ Total de linhas: {total_rows}")

    print(f"\nðŸ“Š Nulos por coluna (valores e %):")

    nulls_df = df.select([
        count(when(col(c).isNull() | isnan(c), c)).alias(c)
        for c in df.columns
    ])

    # Converte o resultado de uma linha para um formato coluna/valor
    for row in nulls_df.collect():
        for col_name in df.columns:
            null_count = row[col_name]
            null_percent = (null_count / total_rows) * 100 if total_rows else 0
            print(f"â€“ {col_name}: {null_count} nulos ({null_percent:.2f}%)")

    print(f"\nðŸ”Ž Amostra de {name}:")
    df.show(5, truncate=False)


def value_counts(df: DataFrame, column: str, show_nulls=True) -> None:
    """
    Exibe a contagem e porcentagem de cada valor Ãºnico em uma coluna.

    ParÃ¢metros:
        df (DataFrame): o DataFrame do Spark
        column (str): o nome da coluna a ser analisada
        show_nulls (bool): se True, inclui valores nulos na contagem
    """
    total = df.count()

    # Cria nova coluna temporÃ¡ria com valor "NULL" para facilitar agrupamento
    col_expr = when(col(column).isNull(), "NULL").otherwise(
        col(column)) if show_nulls else col(column)

    print(f"\nðŸ“Š DistribuiÃ§Ã£o da coluna: {column} (total: {total} registros)")

    df.groupBy(col_expr.alias(column)) \
      .agg(
          count("*").alias("count"),
          (count("*") / lit(total) * 100).alias("percent")
    ) \
        .orderBy("count", ascending=False) \
        .show(truncate=False)

