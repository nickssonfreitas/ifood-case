from pyspark.sql.functions import col, when, mean
from pyspark.sql import DataFrame
from pyspark.sql.functions import col, count, isnan, when, lit


def isna_sum(df: DataFrame, name: str) -> None:
    """
    Exibe o schema, total de linhas e contagem de nulos + percentual por coluna.

    ParÃ¢metros:
        df (DataFrame): DataFrame do Spark
        name (str): Nome para exibiÃ§Ã£o
    """
    total_rows = df.count()

    print(f"\nğŸ“˜ Schema de {name}:")
    df.printSchema()

    print(f"\nğŸ”¢ Total de linhas: {total_rows}")

    print(f"\nğŸ“Š Nulos por coluna (valores e %):")

    nulls_df = df.select([
        count(when(col(c).isNull() | isnan(c), c)).alias(c)
        for c in df.columns
    ])

    # Converte o resultado de uma linha para um formato coluna/valor
    for row in nulls_df.collect():
        for col_name in df.columns:
            null_count = row[col_name]
            null_percent = (null_count / total_rows) * 100 if total_rows else 0
            print(f"â€“ {col_name}: {null_count} nulos ({null_percent:.2f}%)")

    print(f"\nğŸ” Amostra de {name}:")
    df.show(5, truncate=False)


def value_counts(df: DataFrame, column: str, show_nulls=True) -> None:
    """
    Exibe a contagem e porcentagem de cada valor Ãºnico em uma coluna.

    ParÃ¢metros:
        df (DataFrame): o DataFrame do Spark
        column (str): o nome da coluna a ser analisada
        show_nulls (bool): se True, inclui valores nulos na contagem
    """
    total = df.count()

    # Cria nova coluna temporÃ¡ria com valor "NULL" para facilitar agrupamento
    col_expr = when(col(column).isNull(), "NULL").otherwise(
        col(column)) if show_nulls else col(column)

    print(f"\nğŸ“Š DistribuiÃ§Ã£o da coluna: {column} (total: {total} registros)")

    df.groupBy(col_expr.alias(column)) \
      .agg(
          count("*").alias("count"),
          (count("*") / lit(total) * 100).alias("percent")
    ) \
        .orderBy("count", ascending=False) \
        .show(truncate=False)


def clean_customers_data(df: DataFrame) -> DataFrame:
    """
    Cleans the 'gender' and 'credit_card_limit' columns in the customers dataset.

    Cleaning rules:
    - 'gender':
        - Keep 'M' and 'F'
        - Replace 'O' and NULL with 'unknown'
    - 'credit_card_limit':
        - Replace NULLs with the median value of the column

    Parameters:
        df (DataFrame): Input Spark DataFrame with raw customer data

    Returns:
        DataFrame: Cleaned DataFrame
    """

    print("ğŸ” Cleaning 'gender' column...")
    df = df.withColumn(
        "gender",
        when(col("gender").isin("M", "F"), col("gender")).otherwise("unknown")
    )
    print("âœ… 'gender' cleaned: values normalized (M, F, unknown)")

    print("\nğŸ“ˆ Calculating statistics for 'credit_card_limit'...")

    # Mean (optional debug/info)
    mean_value = df.select(mean("credit_card_limit")).first()[0]
    print(f"ğŸ“Š Mean credit limit: {mean_value:.2f}")

    # Median via approxQuantile
    median_value = df.approxQuantile("credit_card_limit", [0.5], 0.01)[0]
    print(f"ğŸ“ Median credit limit: {median_value:.2f}")

    # Fill NULLs with median
    df = df.fillna({"credit_card_limit": median_value})
    print("âœ… 'credit_card_limit' nulls filled with median.")

    return df
